{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddd9TyaiJ6tl"
      },
      "outputs": [],
      "source": [
        "# hangman game\n",
        "def start_game(word=\"hello\", miss_times=6):\n",
        "  word_length = len(word)\n",
        "  try_times = word_length + miss_times\n",
        "  current_state = [\"_\"] * word_length\n",
        "  return word_length, try_times, current_state\n",
        "\n",
        "def update(word, current_state, used_letters):\n",
        "  f = False\n",
        "  for i in range(len(word)):\n",
        "    ch = word[i]\n",
        "    if ch in used_letters and current_state[i]!=ch:\n",
        "      current_state[i] = ch\n",
        "      f = True\n",
        "  return f\n",
        "\n",
        "def evaluate(current_state):\n",
        "  blanks = current_state.count(\"_\")\n",
        "  return blanks\n",
        "\n",
        "def play_game(word, guess_function):\n",
        "  word_length, try_times, current_state = start_game(word, 6) # step 1\n",
        "  char_set = {'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'}\n",
        "  history, current_times, winning = [], 0, False\n",
        "\n",
        "  while current_times < try_times:\n",
        "    c = guess_function(word_length, history, current_state, char_set)\n",
        "    history.append(c) # add in the newly guessed char\n",
        "    char_set.remove(c)\n",
        "\n",
        "    f = update( word, current_state, set(history) ) # step 2\n",
        "    #if f: print(\"guess correctly\")\n",
        "    #else: print(\"no such char\")\n",
        "\n",
        "    blanks = evaluate(current_state) # step 3\n",
        "    if blanks==0:\n",
        "      winning = True\n",
        "      break\n",
        "    current_times += 1\n",
        "  return winning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_path):\n",
        "  f = open(file_path, 'r')\n",
        "  lns = []\n",
        "  for ln in f: lns.append(ln.strip())\n",
        "  return lns\n",
        "\n",
        "# dict: key-value, the value is the frequency\n",
        "# convert the frequencies to probabilities\n",
        "# for example: {'a':1, 'b':2, 'c':3, 'd': 4} -> {'a':0.1, 'b':0.2, 'c':0.3, 'd': 0.4}\n",
        "def normalize_dic(dic):\n",
        "  for k in dic:\n",
        "    S = sum(dic[k].values())\n",
        "    for t in dic[k]: dic[k][t] = dic[k][t]/S\n",
        "  return\n",
        "\n",
        "# group words by length\n",
        "# for words of length L, count the frequency of each char, then sort in order by frequency\n",
        "# set default frequency for each char as 1 in case of 0 if normalization\n",
        "def statistics01(train_words):\n",
        "  default_char_frequency = {'a':1,'b':1,'c':1,'d':1,'e':1,\n",
        "                'f':1,'g':1,'h':1,'i':1,'j':1,\n",
        "                'k':1,'l':1,'m':1,'n':1,'o':1,\n",
        "                'p':1,'q':1,'r':1,'s':1,'t':1,\n",
        "                'u':1,'v':1,'w':1,'x':1,'y':1,\n",
        "                'z':1}\n",
        "  length_char_frequency = {}\n",
        "  for word in train_words:\n",
        "    L = len(word)\n",
        "    if L not in length_char_frequency:\n",
        "      length_char_frequency[L] = default_char_frequency.copy()\n",
        "    for ch in word: length_char_frequency[L][ch] += 1\n",
        "  char_in_order = {}\n",
        "  for k in sorted(length_char_frequency.keys()):\n",
        "    sub = sorted(length_char_frequency[k].items(), key=lambda x:x[1], reverse=True)\n",
        "    sub = list( map(lambda x:x[0], sub) )\n",
        "    char_in_order[k] = sub\n",
        "  return length_char_frequency, char_in_order\n",
        "\n",
        "# count the frequency of each char for the whole training dataset\n",
        "def pattern00(train_words):\n",
        "  dic_1_1 = {'a':0,'b':0,'c':0,'d':0,'e':0,\n",
        "        'f':0,'g':0,'h':0,'i':0,'j':0,\n",
        "        'k':0,'l':0,'m':0,'n':0,'o':0,\n",
        "        'p':0,'q':0,'r':0,'s':0,'t':0,\n",
        "        'u':0,'v':0,'w':0,'x':0,'y':0,\n",
        "        'z':0}\n",
        "  for word in train_words:\n",
        "    for ch in word: dic_1_1[ch] += 1\n",
        "  # normalize\n",
        "  S = sum(dic_1_1.values())\n",
        "  for k in dic_1_1: dic_1_1[k] = dic_1_1[k] / S\n",
        "  return dic_1_1\n",
        "\n",
        "# 2 gram:\n",
        "  # _*: dic_2_1\n",
        "  # *_: dic_2_2\n",
        "def pattern01(train_words):\n",
        "  dic_2_1, dic_2_2 = {}, {}\n",
        "  for w in train_words:\n",
        "    L = len(w)\n",
        "    for i in range( L ):\n",
        "      p0 = w[i]\n",
        "      if i<L-1:\n",
        "        p2 = w[i+1] # _*\n",
        "        if p2 not in dic_2_1: dic_2_1[p2] = {}\n",
        "        if p0 not in dic_2_1[p2]: dic_2_1[p2][p0] = 0\n",
        "        dic_2_1[p2][p0] += 1\n",
        "      if i>0:\n",
        "        p1 = w[i-1] # *_\n",
        "        if p1 not in dic_2_2: dic_2_2[p1] = {}\n",
        "        if p0 not in dic_2_2[p1]: dic_2_2[p1][p0] = 0\n",
        "        dic_2_2[p1][p0] += 1\n",
        "  normalize_dic(dic_2_1)\n",
        "  normalize_dic(dic_2_2)\n",
        "  return dic_2_1, dic_2_2\n",
        "\n",
        "# 3 gram:\n",
        "  # _**: dic_3_1\n",
        "  # *_*: dic_3_2\n",
        "  # **_: dic_3_3\n",
        "def pattern02(train_words):\n",
        "  dic_3_1, dic_3_2, dic_3_3 = {}, {}, {}\n",
        "  for w in train_words:\n",
        "    L = len(w)\n",
        "    for i in range( L ):\n",
        "      p0 = w[i]\n",
        "      if i<L-2:\n",
        "        p1, p2 = w[i+1], w[i+2] # _**\n",
        "        if (p1,p2) not in dic_3_1: dic_3_1[(p1,p2)] = {}\n",
        "        if p0 not in dic_3_1[(p1,p2)]: dic_3_1[(p1,p2)][p0] = 0\n",
        "        dic_3_1[(p1,p2)][p0] += 1\n",
        "      if i>0 and i<L-1:\n",
        "        p1, p2 = w[i-1], w[i+1] # *_*\n",
        "        if (p1,p2) not in dic_3_2: dic_3_2[(p1,p2)] = {}\n",
        "        if p0 not in dic_3_2[(p1,p2)]: dic_3_2[(p1,p2)][p0] = 0\n",
        "        dic_3_2[(p1,p2)][p0] += 1\n",
        "      if i>1:\n",
        "        p1, p2 = w[i-2], w[i-1] # **_\n",
        "        if (p1,p2) not in dic_3_3: dic_3_3[(p1,p2)] = {}\n",
        "        if p0 not in dic_3_3[(p1,p2)]: dic_3_3[(p1,p2)][p0] = 0\n",
        "        dic_3_3[(p1,p2)][p0] += 1\n",
        "  normalize_dic(dic_3_1)\n",
        "  normalize_dic(dic_3_2)\n",
        "  normalize_dic(dic_3_3)\n",
        "  return dic_3_1, dic_3_2, dic_3_3\n",
        "\n",
        "# 4 gram:\n",
        "  # _***: dic_4_1\n",
        "  # *_**: dic_4_2\n",
        "  # **_*: dic_4_3\n",
        "  # ***_: dic_4_4\n",
        "def pattern03(train_words):\n",
        "  dic_4_1, dic_4_2, dic_4_3, dic_4_4 = {}, {}, {}, {}\n",
        "  for w in train_words:\n",
        "    L = len(w)\n",
        "    for i in range( L ):\n",
        "      p0 = w[i]\n",
        "      if i<L-3:\n",
        "        p1, p2, p3 = w[i+1], w[i+2], w[i+3] # _***\n",
        "        if (p1, p2, p3) not in dic_4_1: dic_4_1[(p1, p2, p3)] = {}\n",
        "        if p0 not in dic_4_1[(p1, p2, p3)]: dic_4_1[(p1, p2, p3)][p0] = 0\n",
        "        dic_4_1[(p1, p2, p3)][p0] += 1\n",
        "      if i>0 and i<L-2:\n",
        "        p1, p2, p3 = w[i-1], w[i+1], w[i+2] # *_**\n",
        "        if (p1, p2, p3) not in dic_4_2: dic_4_2[(p1, p2, p3)] = {}\n",
        "        if p0 not in dic_4_2[(p1, p2, p3)]: dic_4_2[(p1, p2, p3)][p0] = 0\n",
        "        dic_4_2[(p1, p2, p3)][p0] += 1\n",
        "      if i>1 and i<L-1:\n",
        "        p1, p2, p3 = w[i-2], w[i-1], w[i+1] # **_*\n",
        "        if (p1, p2, p3) not in dic_4_3: dic_4_3[(p1, p2, p3)] = {}\n",
        "        if p0 not in dic_4_3[(p1, p2, p3)]: dic_4_3[(p1, p2, p3)][p0] = 0\n",
        "        dic_4_3[(p1, p2, p3)][p0] += 1\n",
        "      if i>2:\n",
        "        p1, p2, p3 = w[i-3], w[i-2], w[i-1] # ***_\n",
        "        if (p1, p2, p3) not in dic_4_4: dic_4_4[(p1, p2, p3)] = {}\n",
        "        if p0 not in dic_4_4[(p1, p2, p3)]: dic_4_4[(p1, p2, p3)][p0] = 0\n",
        "        dic_4_4[(p1, p2, p3)][p0] += 1\n",
        "\n",
        "  normalize_dic(dic_4_1)\n",
        "  normalize_dic(dic_4_2)\n",
        "  normalize_dic(dic_4_3)\n",
        "  normalize_dic(dic_4_4)\n",
        "  return dic_4_1, dic_4_2, dic_4_3, dic_4_4\n",
        "\n",
        "# 5 gram\n",
        "def pattern04(train_words):\n",
        "  dic_5_1, dic_5_2, dic_5_3, dic_5_4, dic_5_5 = {}, {}, {}, {}, {}\n",
        "  for w in train_words:\n",
        "    L = len(w)\n",
        "    for i in range( L ):\n",
        "      p0 = w[i]\n",
        "      if i-4>=0: # ****_\n",
        "        p1, p2, p3, p4 = w[i-4], w[i-3], w[i-2], w[i-1]\n",
        "        if (p1, p2, p3, p4) not in dic_5_5: dic_5_5[(p1, p2, p3, p4)] = {}\n",
        "        if p0 not in dic_5_5[(p1, p2, p3, p4)]: dic_5_5[(p1, p2, p3, p4)][p0] = 0\n",
        "        dic_5_5[(p1, p2, p3, p4)][p0] += 1\n",
        "      if i-3>=0 and i+1<L: # ***_*\n",
        "        p1, p2, p3, p4 = w[i-3], w[i-2], w[i-1], w[i+1]\n",
        "        if (p1, p2, p3, p4) not in dic_5_4: dic_5_4[(p1, p2, p3, p4)] = {}\n",
        "        if p0 not in dic_5_4[(p1, p2, p3, p4)]: dic_5_4[(p1, p2, p3, p4)][p0] = 0\n",
        "        dic_5_4[(p1, p2, p3, p4)][p0] += 1\n",
        "      if i-2>=0 and i+2<L: # **_**\n",
        "        p1, p2, p3, p4 = w[i-2], w[i-1], w[i+1], w[i+2]\n",
        "        if (p1, p2, p3, p4) not in dic_5_3: dic_5_3[(p1, p2, p3, p4)] = {}\n",
        "        if p0 not in dic_5_3[(p1, p2, p3, p4)]: dic_5_3[(p1, p2, p3, p4)][p0] = 0\n",
        "        dic_5_3[(p1, p2, p3, p4)][p0] += 1\n",
        "      if i-1>=0 and i+3<L: # *_***\n",
        "        p1, p2, p3, p4 = w[i-1], w[i+1], w[i+2], w[i+3]\n",
        "        if (p1, p2, p3, p4) not in dic_5_2: dic_5_2[(p1, p2, p3, p4)] = {}\n",
        "        if p0 not in dic_5_2[(p1, p2, p3, p4)]: dic_5_2[(p1, p2, p3, p4)][p0] = 0\n",
        "        dic_5_2[(p1, p2, p3, p4)][p0] += 1\n",
        "      if i+4<L: # _****\n",
        "        p1, p2, p3, p4 = w[i+1], w[i+2], w[i+3], w[i+4]\n",
        "        if (p1, p2, p3, p4) not in dic_5_1: dic_5_1[(p1, p2, p3, p4)] = {}\n",
        "        if p0 not in dic_5_1[(p1, p2, p3, p4)]: dic_5_1[(p1, p2, p3, p4)][p0] = 0\n",
        "        dic_5_1[(p1, p2, p3, p4)][p0] += 1\n",
        "\n",
        "  normalize_dic(dic_5_1)\n",
        "  normalize_dic(dic_5_2)\n",
        "  normalize_dic(dic_5_3)\n",
        "  normalize_dic(dic_5_4)\n",
        "  normalize_dic(dic_5_5)\n",
        "  return dic_5_1, dic_5_2, dic_5_3, dic_5_4, dic_5_5"
      ],
      "metadata": {
        "id": "p5Yj8A9KV2w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# for dict: key is char, and value is the probability (or frequency)\n",
        "# choose the char with the highest probability (or frequency)\n",
        "def choose_high_frequency(char_fre):\n",
        "  c, f = ' ', -1\n",
        "  for k in char_fre:\n",
        "    if char_fre[k]>f: c, f = k, char_fre[k]\n",
        "  return c\n",
        "\n",
        "# merge two dict\n",
        "# use the common key, and value is the sum\n",
        "def merge_two_dic01(d1, d2):\n",
        "  d = {}\n",
        "  for k in d1.keys():\n",
        "    if k in d2: d[k] = d1[k] + d2[k]\n",
        "  return d\n",
        "\n",
        "# check for n-gram\n",
        "def check_list(current_state, lst):\n",
        "  for i in lst:\n",
        "    if current_state[i] == '_': return False\n",
        "  return True\n",
        "\n",
        "# 2 gram:\n",
        "  # *_\n",
        "  # _*\n",
        "# 3 gram:\n",
        "  # **_\n",
        "  # *_*\n",
        "  # _**\n",
        "# 4 gram:\n",
        "  # **_*\n",
        "  # ***_\n",
        "  # *_**\n",
        "  # _***\n",
        "# 5 gram:\n",
        "  # **_**\n",
        "  # ***_*\n",
        "  # ****_\n",
        "  # *_***\n",
        "  # _****\n",
        "def N_grams(current_state, word_length):\n",
        "  # dic_2_1, dic_2_2,\n",
        "  # dic_3_1, dic_3_2, dic_3_3\n",
        "  # dic_4_1, dic_4_2, dic_4_3, dic_4_4\n",
        "  # dic_5_1, dic_5_2, dic_5_3, dic_5_4, dic_5_5\n",
        "\n",
        "  gram2, gram3, gram4 = {}, {}, {}\n",
        "  gram5 = {}\n",
        "  for i in range(word_length):\n",
        "    if current_state[i]=='_':\n",
        "      # 5 gram:\n",
        "      if i-4>=0: # ****_\n",
        "        if check_list(current_state, [i-4,i-3,i-2,i-1]):\n",
        "          p1, p2, p3, p4 = current_state[i-4], current_state[i-3], current_state[i-2], current_state[i-1]\n",
        "          if (p1, p2, p3, p4) in dic_5_5:\n",
        "            if not gram5: gram5 = dic_5_5[(p1, p2, p3, p4)]\n",
        "            else: gram5 = merge_two_dic01(gram5, dic_5_5[(p1, p2, p3, p4)])\n",
        "      if i-3>=0 and i+1<word_length: # ***_*\n",
        "        if check_list(current_state, [i-3,i-2,i-1,i+1]):\n",
        "          p1, p2, p3, p4 = current_state[i-3], current_state[i-2], current_state[i-1], current_state[i+1]\n",
        "          if (p1, p2, p3, p4) in dic_5_4:\n",
        "            if not gram5: gram5 = dic_5_4[(p1, p2, p3, p4)]\n",
        "            else: gram5 = merge_two_dic01(gram5, dic_5_4[(p1, p2, p3, p4)])\n",
        "      if i-2>=0 and i+2<word_length: # **_**\n",
        "        if check_list(current_state, [i-2,i-1,i+1,i+2]):\n",
        "          p1, p2, p3, p4 = current_state[i-2], current_state[i-1], current_state[i+1], current_state[i+2],\n",
        "          if (p1, p2, p3, p4) in dic_5_3:\n",
        "            if not gram5: gram5 = dic_5_3[(p1, p2, p3, p4)]\n",
        "            else: gram5 = merge_two_dic01(gram5, dic_5_3[(p1, p2, p3, p4)])\n",
        "      if i-1>=0 and i+3<word_length: # *_***\n",
        "        if check_list(current_state, [i-1,i+1,i+2,i+3]):\n",
        "          p1, p2, p3, p4 = current_state[i-1], current_state[i+1], current_state[i+2], current_state[i+3]\n",
        "          if (p1, p2, p3, p4) in dic_5_2:\n",
        "            if not gram5: gram5 = dic_5_2[(p1, p2, p3, p4)]\n",
        "            else: gram5 = merge_two_dic01(gram5, dic_5_2[(p1, p2, p3, p4)])\n",
        "      if i+4<word_length: # _****\n",
        "        if check_list(current_state, [i+1,i+2,i+3,i+4]):\n",
        "          p1, p2, p3, p4 = current_state[i+1], current_state[i+2], current_state[i+3], current_state[i+4]\n",
        "          if (p1, p2, p3, p4) in dic_5_1:\n",
        "            if not gram5: gram5 = dic_5_1[(p1, p2, p3, p4)]\n",
        "            else: gram5 = merge_two_dic01(gram5, dic_5_1[(p1, p2, p3, p4)])\n",
        "\n",
        "      # 4 gram: ***_\n",
        "      if i-3>=0 and current_state[i-3]!='_' and current_state[i-2]!='_' and current_state[i-1]!='_':\n",
        "        p1, p2, p3 = current_state[i-3], current_state[i-2], current_state[i-1]\n",
        "        if (p1, p2, p3) in dic_4_4:\n",
        "          if not gram4: gram4 = dic_4_4[(p1, p2, p3)]\n",
        "          else: gram4 = merge_two_dic01(gram4, dic_4_4[(p1, p2, p3)])\n",
        "      # 4 gram: **_*\n",
        "      if i-2>=0 and i+1<word_length and current_state[i-2]!='_' and current_state[i-1]!='_' and current_state[i+1]!='_' :\n",
        "        p1, p2, p3 = current_state[i-2], current_state[i-1], current_state[i+1]\n",
        "        if (p1, p2, p3) in dic_4_3:\n",
        "          if not gram4: gram4 = dic_4_3[(p1, p2, p3)]\n",
        "          else: gram4 = merge_two_dic01(gram4, dic_4_3[(p1, p2, p3)])\n",
        "      # 4 gram: *_**\n",
        "      if i-1>=0 and i+2<word_length and current_state[i-1]!='_' and current_state[i+1]!='_' and current_state[i+2]!='_' :\n",
        "        p1, p2, p3 = current_state[i-1], current_state[i+1], current_state[i+2]\n",
        "        if (p1, p2, p3) in dic_4_2:\n",
        "          if not gram4: gram4 = dic_4_2[(p1, p2, p3)]\n",
        "          else: gram4 = merge_two_dic01(gram4, dic_4_2[(p1, p2, p3)])\n",
        "      # 4 gram: _***\n",
        "      if i+3<word_length and current_state[i+1]!='_' and current_state[i+2]!='_' and current_state[i+3]!='_' :\n",
        "        p1, p2, p3 = current_state[i+1], current_state[i+2], current_state[i+3]\n",
        "        if (p1, p2, p3) in dic_4_1:\n",
        "          if not gram4: gram4 = dic_4_1[(p1, p2, p3)]\n",
        "          else: gram4 = merge_two_dic01(gram4, dic_4_1[(p1, p2, p3)])\n",
        "\n",
        "      # 3 gram: **_\n",
        "      if i-2>=0 and current_state[i-2]!='_' and current_state[i-1]!='_':\n",
        "        p1, p2 = current_state[i-2], current_state[i-1]\n",
        "        if (p1, p2) in dic_3_3:\n",
        "          if not gram3: gram3 = dic_3_3[(p1, p2)]\n",
        "          else: gram3 = merge_two_dic01(gram3, dic_3_3[(p1, p2)])\n",
        "      # 3 gram: *_*\n",
        "      if i-1>=0 and current_state[i-1]!='_' and i+1<word_length and current_state[i+1]!='_':\n",
        "        p1, p2 = current_state[i-1], current_state[i+1]\n",
        "        if (p1, p2) in dic_3_2:\n",
        "          if not gram3: gram3 = dic_3_2[(p1, p2)]\n",
        "          else: gram3 = merge_two_dic01(gram3, dic_3_2[(p1, p2)])\n",
        "      # 3 gram: _**\n",
        "      if i+2<word_length and current_state[i+1]!='_' and current_state[i+2]!='_':\n",
        "        p1, p2 = current_state[i+1], current_state[i+2]\n",
        "        if (p1, p2) in dic_3_1:\n",
        "          if not gram3: gram3 = dic_3_1[(p1, p2)]\n",
        "          else: gram3 = merge_two_dic01(gram3, dic_3_1[(p1, p2)])\n",
        "\n",
        "      # 2 gram: *_\n",
        "      if i-1>=0 and current_state[i-1]!='_':\n",
        "        p1 = current_state[i-1]\n",
        "        if p1 in dic_2_2:\n",
        "          if not gram2: gram2 = dic_2_2[p1]\n",
        "          else: gram2 = merge_two_dic01(gram2, dic_2_2[p1])\n",
        "      # 2 gram: _*\n",
        "      if i+1<word_length and current_state[i+1]!='_':\n",
        "        p2 = current_state[i+1]\n",
        "        if p2 in dic_2_1:\n",
        "          if not gram2: gram2 = dic_2_1[p2]\n",
        "          else: gram2 = merge_two_dic01(gram2, dic_2_1[p2])\n",
        "  return gram2, gram3, gram4, gram5\n",
        "\n",
        "def previous_info(current_state, history, word_length):\n",
        "  prev_state = current_state.copy()\n",
        "  prev_guess = history[-1]\n",
        "  label = 0\n",
        "  for i in range(word_length):\n",
        "    if prev_state[i] == prev_guess:\n",
        "      prev_state[i] = '_'\n",
        "      label = 1\n",
        "  return label, prev_state, prev_guess\n",
        "\n",
        "def state_to_vector(current_state, history):\n",
        "  #dic = {'a':0,'b':0,'c':0,'d':0,'e':0,'f':0,'g':0,'h':0,'i':0,'j':0,'k':0,'l':0,'m':0,'n':0,'o':0,'p':0,'q':0,'r':0,'s':0,'t':0,'u':0,'v':0,'w':0,'x':0,'y':0,'z':0,'_':0}\n",
        "  # length is 27: a-z, _\n",
        "  vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "  for c in current_state:\n",
        "    if c=='_': vec[-1] += 1\n",
        "    else: vec[ ord(c)-ord('a') ] += 1\n",
        "  right_num = len(current_state) - vec[-1]\n",
        "  wrong_num = 0\n",
        "  for c in history:\n",
        "    if c not in current_state: wrong_num += 1\n",
        "  #for c in history:\n",
        "    #vec[ ord(c)-ord('a') ] = -1\n",
        "  return vec + [6-wrong_num]\n",
        "\n",
        "def char_to_vector(c):\n",
        "  #dic = {'a':0,'b':0,'c':0,'d':0,'e':0,'f':0,'g':0,'h':0,'i':0,'j':0,'k':0,'l':0,'m':0,'n':0,'o':0,'p':0,'q':0,'r':0,'s':0,'t':0,'u':0,'v':0,'w':0,'x':0,'y':0,'z':0}\n",
        "  vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "  if c==\"_\": return vec\n",
        "  vec[ ord(c)-ord('a') ] = 1\n",
        "  return vec\n",
        "\n",
        "def dic_to_list(grams):\n",
        "  vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "  for k in grams:\n",
        "    vec[ ord(k)-ord('a') ] = grams[k]\n",
        "  return vec\n",
        "\n",
        "def guess_char(word_length, history, current_state, char_set):\n",
        "  #\n",
        "  # use ML model\n",
        "  use_model = False # True, False\n",
        "\n",
        "  if history and not use_model:\n",
        "    label, prev_state, prev_guess = previous_info(current_state, history, word_length)\n",
        "    prev_gram2, prev_gram3, prev_gram4, prev_gram5 = N_grams(prev_state, word_length)\n",
        "    prev_grams = merge_grams(prev_gram2, prev_gram3, prev_gram4, prev_gram5, history[:-1])\n",
        "    vec1 = state_to_vector(prev_state, history[:-1])\n",
        "    vec2 = dic_to_list(prev_grams)\n",
        "    vec3 = char_to_vector(prev_guess)\n",
        "    if label==1:\n",
        "      global training_X, training_y\n",
        "      training_X.append( vec1+[word_length]+vec2 )\n",
        "      training_y.append( vec3 )\n",
        "\n",
        "    \"\"\"\n",
        "    print( \"previous state & guess: \", label, prev_state, prev_guess )\n",
        "    print( vec1 )\n",
        "    print( vec2 )\n",
        "    print( vec3 )\n",
        "    print()\n",
        "    \"\"\"\n",
        "\n",
        "  possible = []\n",
        "  c = ' '\n",
        "  #c = random.choices( ['a', 'o', 'e', 'i', 'u'] )[0]\n",
        "  preferred_dic = char_in_order_by_default.copy()\n",
        "  if word_length in char_in_order_by_length:\n",
        "    preferred_dic = char_in_order_by_length[word_length].copy()\n",
        "  for ch in history:\n",
        "    if ch in preferred_dic: preferred_dic.remove(ch)\n",
        "  possible = preferred_dic.copy()\n",
        "  c = preferred_dic.pop(0)\n",
        "\n",
        "    # the first guess\n",
        "  #if not history: return c\n",
        "\n",
        "    # n-grams\n",
        "  gram2, gram3, gram4, gram5 = N_grams(current_state, word_length)\n",
        "  grams = merge_grams(gram2, gram3, gram4, gram5, history)\n",
        "  possible = grams\n",
        "\n",
        "  #if word_length>=12: gram4, gram5 = {}, {}\n",
        "  #else: gram5 = {}\n",
        "  #print(c, history, current_state)\n",
        "  #print( sorted(grams.items(), key=lambda x:x[1], reverse=True) )\n",
        "  #if gram2: #  or gram3 or gram4 or gram5\n",
        "  c = choose_high_frequency(grams)\n",
        "\n",
        "\n",
        "  if use_model:\n",
        "    global clf\n",
        "    vec1 = state_to_vector(current_state, history)\n",
        "    vec2 = dic_to_list(grams)\n",
        "    features = vec1 + [word_length] + vec2\n",
        "    chs = clf.predict_proba( [features] )[0]\n",
        "    for ch in history:\n",
        "      chs[ ord(ch)-ord('a') ] = 0\n",
        "\n",
        "    if max(chs) >= 0.0:\n",
        "      pred = chr( np.argmax( chs ) + 97 )\n",
        "      #print( \"pred: \\t\", pred )\n",
        "      #if pred in grams:\n",
        "      c = pred\n",
        "  #print(c, history, current_state)\n",
        "  return c\n",
        "\n",
        "def merge_grams(gram2, gram3, gram4, gram5, history):\n",
        "  grams = dic_1_1.copy()\n",
        "\n",
        "  for k in grams:\n",
        "    grams[k] = grams[k] * 0.3 # 0.5, 0.8\n",
        "    if k in gram2: grams[k] += gram2[k]*0.5\n",
        "    if k in gram3: grams[k] += gram3[k]*1\n",
        "    if k in gram4: grams[k] += gram4[k]*2 #2.5\n",
        "    if k in gram5: grams[k] += gram5[k]*8 #5-8\n",
        "  for ch in history:\n",
        "    if ch in grams: del grams[ch]\n",
        "  return grams\n",
        "\n",
        "def weeken(grams, unlikely_dic, word_length):\n",
        "  count = 0\n",
        "  for (k,v) in sorted(unlikely_dic.items(), key=lambda x:x[1], reverse=True):\n",
        "    if k in grams:\n",
        "      grams[k] -= v*0.5\n",
        "      #grams[k] = grams[k] * weight0\n",
        "    count += 1\n",
        "    if count>word_length//2: break\n",
        "  return"
      ],
      "metadata": {
        "id": "xOSlvtVmZbRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-3FIQYKd6kY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/words_250000_train.txt\"\n",
        "whole_words = read_file(file_path)\n",
        "random.shuffle(whole_words) # randomize those words\n",
        "\n",
        "training_X, training_y = [], []\n",
        "\n",
        "for split_ratio in [0.8]: # 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95\n",
        "  Len = int(len(whole_words)*split_ratio)\n",
        "  train_words = whole_words[:Len]\n",
        "  test_words = whole_words[Len:]\n",
        "\n",
        "  # statistics of the training data, calculate probabilities for n-grams\n",
        "  dic_1_1 = pattern00(train_words)\n",
        "  dic_2_1, dic_2_2 = pattern01(train_words)\n",
        "  dic_3_1, dic_3_2, dic_3_3 = pattern02(train_words)\n",
        "  dic_4_1, dic_4_2, dic_4_3, dic_4_4 = pattern03(train_words)\n",
        "  dic_5_1, dic_5_2, dic_5_3, dic_5_4, dic_5_5 = pattern04(train_words)\n",
        "\n",
        "  len_dic, char_in_order_by_length = statistics01(train_words)\n",
        "  char_in_order_by_default = sorted(dic_1_1.items(), key=lambda x:x[1], reverse=True)\n",
        "  char_in_order_by_default = list(map(lambda x:x[0], char_in_order_by_default))\n",
        "\n",
        "  # training data\n",
        "  c1, c2 = 0, 0\n",
        "  for word in train_words[:]:\n",
        "    #print(word)\n",
        "    winning = play_game(word, guess_char)\n",
        "    if winning:\n",
        "      c2 += 1\n",
        "      #print(word)\n",
        "    c1 += 1\n",
        "  print( \"training vs testing \\t\", Len, \" vs\", len(whole_words)-Len )\n",
        "  print( \"\\t\", round(split_ratio*100), \"% vs\", round((1-split_ratio)*100), \"%\" )\n",
        "  print( \"\\t\", \"testing accuracy: \", round(c2/c1*100, 2), \"%\", \"\\t\", c1, c2 )\n",
        "\n",
        "len(training_X), len(training_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZAxLibBgUGl",
        "outputId": "d24fcc4e-9d42-40c4-cfdb-af21b2e12746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training vs testing \t 181840  vs 45460\n",
            "\t 80 % vs 20 %\n",
            "\t testing accuracy:  78.46 % \t 181840 142679\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1126107, 1126107)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "clf = MLPClassifier(max_iter=10, alpha=1e-5,\n",
        "                    hidden_layer_sizes=(256, 128, 64, 32),\n",
        "                    verbose=True, random_state=1)\n",
        "clf.fit( training_X, training_y )\n",
        "clf.score( training_X, training_y )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGQVwXpxIY8B",
        "outputId": "f1243a3f-6151-4ca9-e4c6-a496bc768a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.70613355\n",
            "Iteration 2, loss = 0.26782214\n",
            "Iteration 3, loss = 0.20010905\n",
            "Iteration 4, loss = 0.16849337\n",
            "Iteration 5, loss = 0.15003835\n",
            "Iteration 6, loss = 0.13614140\n",
            "Iteration 7, loss = 0.12623058\n",
            "Iteration 8, loss = 0.11911023\n",
            "Iteration 9, loss = 0.11029544\n",
            "Iteration 10, loss = 0.10734996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9686104428797618"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing data\n",
        "c1, c2 = 0, 0\n",
        "for word in test_words[:]:\n",
        "  winning = play_game(word, guess_char)\n",
        "  if winning: c2 += 1\n",
        "  c1 += 1\n",
        "print( \"training vs testing \\t\", Len, \" vs\", len(whole_words)-Len )\n",
        "print( \"\\t\", round(split_ratio*100), \"% vs\", round((1-split_ratio)*100), \"%\" )\n",
        "print( \"\\t\", \"testing accuracy: \", round(c2/c1*100, 2), \"%\", \"\\t\", c1, c2 )\n",
        "# N-gram:     testing accuracy:  70.95 % \t 45460 32254\n",
        "# ML model:   testing accuracy:  71.73 % \t 4546 3261"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nd3OVsuIin9",
        "outputId": "7dd5cbca-637e-4896-98c7-ce63aaa56ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training vs testing \t 181840  vs 45460\n",
            "\t 80 % vs 20 %\n",
            "\t testing accuracy:  71.29 % \t 45460 32408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yOhwgdc-GNsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# save\n",
        "with open('model11.pkl','wb') as f:\n",
        "    pickle.dump(clf,f)"
      ],
      "metadata": {
        "id": "jUEJlEunjZJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load\n",
        "with open('model11.pkl', 'rb') as f:\n",
        "    clf2 = pickle.load(f)"
      ],
      "metadata": {
        "id": "iq3TRGBCnhN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for length in [4]: #range(1, 20)\n",
        "  # testing data\n",
        "  c1, c2 = 0, 0\n",
        "  for word in whole_words[:]:  # whole_words, test_words\n",
        "    if len(word)!=length: continue\n",
        "    #print(word)\n",
        "    winning = play_game(word, guess_char)\n",
        "    if winning:\n",
        "      c2 += 1\n",
        "      #print(word)\n",
        "    c1 += 1\n",
        "  #print( \"training vs testing \\t\", Len, \" vs\", len(whole_words)-Len )\n",
        "  #print( \"\\t\", round(split_ratio*100), \"% vs\", round((1-split_ratio)*100), \"%\" )\n",
        "  print( \"\\t\", length, \"\\t\", \"testing accuracy: \", round(c2/c1*100, 2), \"%\", \"\\t\", c1, c2 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQyyAm6pXUFJ",
        "outputId": "635390b3-818b-4d1c-a4ba-05ad9922e80d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t 4 \t testing accuracy:  21.43 % \t 5287 1133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "5 grams with initialization\n",
        "   1 \t testing accuracy:  41.18 % \t 17 7\n",
        "\t 2 \t testing accuracy:  12.88 % \t 264 34\n",
        "\t 3 \t testing accuracy:  12.4 % \t 2201 273\n",
        "\t 4 \t testing accuracy:  21.79 % \t 5287 1152\n",
        "\t 5 \t testing accuracy:  33.25 % \t 11274 3749\n",
        "\t 6 \t testing accuracy:  46.83 % \t 19541 9151\n",
        "\t 7 \t testing accuracy:  60.9 % \t 25948 15802\n",
        "\t 8 \t testing accuracy:  74.42 % \t 30452 22662\n",
        "\t 9 \t testing accuracy:  85.07 % \t 30906 26291\n",
        "\t 10 \t testing accuracy:  91.66 % \t 26953 24704\n",
        "\t 11 \t testing accuracy:  95.94 % \t 22786 21860\n",
        "\t 12 \t testing accuracy:  97.95 % \t 18178 17805\n",
        "\t 13 \t testing accuracy:  98.96 % \t 12956 12821\n",
        "\t 14 \t testing accuracy:  99.44 % \t 8710 8661\n",
        "\t 15 \t testing accuracy:  99.79 % \t 5211 5200\n",
        "\t 16 \t testing accuracy:  99.87 % \t 3143 3139\n",
        "\t 17 \t testing accuracy:  100.0 % \t 1775 1775\n",
        "\t 18 \t testing accuracy:  100.0 % \t 859 859\n",
        "\t 19 \t testing accuracy:  100.0 % \t 441 441\n",
        "5 grams\n",
        "1 \t testing accuracy:  41.18 % \t 17 7\n",
        "2 \t testing accuracy:  12.88 % \t 264 34\n",
        "3 \t testing accuracy:  12.22 % \t 2201 269\n",
        "4 \t testing accuracy:  21.2 % \t 5287 1121\n",
        "5 \t testing accuracy:  31.54 % \t 11274 3556\n",
        "6 \t testing accuracy:  43.82 % \t 19541 8563\n",
        "7 \t testing accuracy:  57.62 % \t 25948 14952\n",
        "8 \t testing accuracy:  70.61 % \t 30452 21502\n",
        "9 \t testing accuracy:  82.35 % \t 30906 25451\n",
        "10 \t testing accuracy:  89.05 % \t 26953 24001\n",
        "11 \t testing accuracy:  93.25 % \t 22786 21248\n",
        "12 \t testing accuracy:  95.76 % \t 18178 17408\n",
        "13 \t testing accuracy:  96.41 % \t 12956 12491\n",
        "14 \t testing accuracy:  97.03 % \t 8710 8451\n",
        "15 \t testing accuracy:  99.06 % \t 5211 5162\n",
        "16 \t testing accuracy:  97.9 % \t 3143 3077\n",
        "17 \t testing accuracy:  98.93 % \t 1775 1756\n",
        "18 \t testing accuracy:  99.65 % \t 859 856\n",
        "19 \t testing accuracy:  100.0 % \t 441 441\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DU0ZcNdzIjeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YimjEwA81jx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YFy2P_Qqlg5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.predict_proba(training_X[:1])[0]\n",
        "chr( np.argmax( clf.predict(training_X[:1])[0] ) + 97 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ml955xA_pF1",
        "outputId": "063dc2b4-8642-4e67-fac8-35278ed2bf92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.15497573e-06, 8.12927773e-08, 9.25120795e-15, 5.07808828e-16,\n",
              "       9.99998780e-01, 4.95202581e-10, 4.42485816e-12, 1.17635321e-08,\n",
              "       6.62618404e-08, 3.93186305e-17, 3.11000742e-09, 2.92719191e-08,\n",
              "       2.99775159e-13, 7.46888890e-09, 3.72784725e-07, 6.45992240e-11,\n",
              "       2.39263130e-11, 1.85475320e-09, 1.39655986e-08, 2.40194363e-08,\n",
              "       4.77673640e-06, 3.40532750e-10, 1.80602430e-10, 6.43457899e-11,\n",
              "       1.94626552e-12, 3.74234504e-11])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "wTudSCGF_6MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play_game(\"hello\", guess_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTpzG1cuCOnR",
        "outputId": "e547dc7c-65d3-4622-e8d6-bff3c19f202f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}