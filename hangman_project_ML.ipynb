{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vz-6-wHXOs8"
      },
      "outputs": [],
      "source": [
        "# hangman game\n",
        "def start_game(word=\"hello\", miss_times=6):\n",
        "  word_length = len(word)\n",
        "  try_times = word_length + miss_times\n",
        "  current_state = [\"_\"] * word_length\n",
        "  return word_length, try_times, current_state\n",
        "\n",
        "def update(word, current_state, used_letters):\n",
        "  f = False\n",
        "  for i in range(len(word)):\n",
        "    ch = word[i]\n",
        "    if ch in used_letters and current_state[i]!=ch:\n",
        "      current_state[i] = ch\n",
        "      f = True\n",
        "  return f\n",
        "\n",
        "def evaluate(current_state):\n",
        "  blanks = current_state.count(\"_\")\n",
        "  return blanks\n",
        "\n",
        "def play_game(word, guess_function):\n",
        "  word_length, try_times, current_state = start_game(word, 6) # step 1\n",
        "  char_set = {'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'}\n",
        "  history, current_times, winning = [], 0, False\n",
        "\n",
        "  while current_times < try_times:\n",
        "    c = guess_function(word_length, history, current_state, char_set)\n",
        "    history.append(c) # add in the newly guessed char\n",
        "    char_set.remove(c)\n",
        "\n",
        "    f = update( word, current_state, set(history) ) # step 2\n",
        "    #if f: print(\"guess correctly\")\n",
        "    #else: print(\"no such char\")\n",
        "\n",
        "    blanks = evaluate(current_state) # step 3\n",
        "    if blanks==0:\n",
        "      winning = True\n",
        "      break\n",
        "    current_times += 1\n",
        "  return winning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_path):\n",
        "  f = open(file_path, 'r')\n",
        "  lns = []\n",
        "  for ln in f: lns.append(ln.strip())\n",
        "  return lns\n",
        "\n",
        "# dict: key-value, the value is the frequency\n",
        "# convert the frequencies to probabilities\n",
        "# for example: {'a':1, 'b':2, 'c':3, 'd': 4} -> {'a':0.1, 'b':0.2, 'c':0.3, 'd': 0.4}\n",
        "def normalize_dic(dic):\n",
        "  for k in dic:\n",
        "    S = sum(dic[k].values())\n",
        "    for t in dic[k]: dic[k][t] = dic[k][t]/S\n",
        "  return\n",
        "\n",
        "# group words by length\n",
        "# for words of length L, count the frequency of each char, then sort in order by frequency\n",
        "# set default frequency for each char as 1 in case of 0 if normalization\n",
        "def statistics01(train_words):\n",
        "  default_char_frequency = {'a':1,'b':1,'c':1,'d':1,'e':1,\n",
        "                'f':1,'g':1,'h':1,'i':1,'j':1,\n",
        "                'k':1,'l':1,'m':1,'n':1,'o':1,\n",
        "                'p':1,'q':1,'r':1,'s':1,'t':1,\n",
        "                'u':1,'v':1,'w':1,'x':1,'y':1,\n",
        "                'z':1}\n",
        "  length_char_frequency = {}\n",
        "  for word in train_words:\n",
        "    L = len(word)\n",
        "    if L not in length_char_frequency:\n",
        "      length_char_frequency[L] = default_char_frequency.copy()\n",
        "    for ch in word: length_char_frequency[L][ch] += 1\n",
        "  char_in_order = {}\n",
        "  for k in sorted(length_char_frequency.keys()):\n",
        "    sub = sorted(length_char_frequency[k].items(), key=lambda x:x[1], reverse=True)\n",
        "    sub = list( map(lambda x:x[0], sub) )\n",
        "    char_in_order[k] = sub\n",
        "  return length_char_frequency, char_in_order\n",
        "\n",
        "# count the frequency of each char for the whole training dataset\n",
        "def pattern00(train_words):\n",
        "  dic_1_1 = {'a':0,'b':0,'c':0,'d':0,'e':0,\n",
        "        'f':0,'g':0,'h':0,'i':0,'j':0,\n",
        "        'k':0,'l':0,'m':0,'n':0,'o':0,\n",
        "        'p':0,'q':0,'r':0,'s':0,'t':0,\n",
        "        'u':0,'v':0,'w':0,'x':0,'y':0,\n",
        "        'z':0}\n",
        "  for word in train_words:\n",
        "    for ch in word: dic_1_1[ch] += 1\n",
        "  # normalize\n",
        "  S = sum(dic_1_1.values())\n",
        "  for k in dic_1_1: dic_1_1[k] = dic_1_1[k] / S\n",
        "  return dic_1_1\n",
        "\n",
        "# 2 gram:\n",
        "  # _*: dic_2_1\n",
        "  # *_: dic_2_2\n",
        "def pattern01(train_words):\n",
        "  dic_2_1, dic_2_2 = {}, {}\n",
        "  for w in train_words:\n",
        "    L = len(w)\n",
        "    for i in range( L ):\n",
        "      p0 = w[i]\n",
        "      if i<L-1:\n",
        "        p2 = w[i+1] # _*\n",
        "        if p2 not in dic_2_1: dic_2_1[p2] = {}\n",
        "        if p0 not in dic_2_1[p2]: dic_2_1[p2][p0] = 0\n",
        "        dic_2_1[p2][p0] += 1\n",
        "      if i>0:\n",
        "        p1 = w[i-1] # *_\n",
        "        if p1 not in dic_2_2: dic_2_2[p1] = {}\n",
        "        if p0 not in dic_2_2[p1]: dic_2_2[p1][p0] = 0\n",
        "        dic_2_2[p1][p0] += 1\n",
        "  normalize_dic(dic_2_1)\n",
        "  normalize_dic(dic_2_2)\n",
        "  return dic_2_1, dic_2_2\n",
        "\n",
        "# 3 gram:\n",
        "  # _**: dic_3_1\n",
        "  # *_*: dic_3_2\n",
        "  # **_: dic_3_3\n",
        "def pattern02(train_words):\n",
        "  dic_3_1, dic_3_2, dic_3_3 = {}, {}, {}\n",
        "  for w in train_words:\n",
        "    L = len(w)\n",
        "    for i in range( L ):\n",
        "      p0 = w[i]\n",
        "      if i<L-2:\n",
        "        p1, p2 = w[i+1], w[i+2] # _**\n",
        "        if (p1,p2) not in dic_3_1: dic_3_1[(p1,p2)] = {}\n",
        "        if p0 not in dic_3_1[(p1,p2)]: dic_3_1[(p1,p2)][p0] = 0\n",
        "        dic_3_1[(p1,p2)][p0] += 1\n",
        "      if i>0 and i<L-1:\n",
        "        p1, p2 = w[i-1], w[i+1] # *_*\n",
        "        if (p1,p2) not in dic_3_2: dic_3_2[(p1,p2)] = {}\n",
        "        if p0 not in dic_3_2[(p1,p2)]: dic_3_2[(p1,p2)][p0] = 0\n",
        "        dic_3_2[(p1,p2)][p0] += 1\n",
        "      if i>1:\n",
        "        p1, p2 = w[i-2], w[i-1] # **_\n",
        "        if (p1,p2) not in dic_3_3: dic_3_3[(p1,p2)] = {}\n",
        "        if p0 not in dic_3_3[(p1,p2)]: dic_3_3[(p1,p2)][p0] = 0\n",
        "        dic_3_3[(p1,p2)][p0] += 1\n",
        "  normalize_dic(dic_3_1)\n",
        "  normalize_dic(dic_3_2)\n",
        "  normalize_dic(dic_3_3)\n",
        "  return dic_3_1, dic_3_2, dic_3_3\n",
        "\n",
        "# 4 gram:\n",
        "  # _***: dic_4_1\n",
        "  # *_**: dic_4_2\n",
        "  # **_*: dic_4_3\n",
        "  # ***_: dic_4_4\n",
        "def pattern03(train_words):\n",
        "  dic_4_1, dic_4_2, dic_4_3, dic_4_4 = {}, {}, {}, {}\n",
        "  for w in train_words:\n",
        "    L = len(w)\n",
        "    for i in range( L ):\n",
        "      p0 = w[i]\n",
        "      if i<L-3:\n",
        "        p1, p2, p3 = w[i+1], w[i+2], w[i+3] # _***\n",
        "        if (p1, p2, p3) not in dic_4_1: dic_4_1[(p1, p2, p3)] = {}\n",
        "        if p0 not in dic_4_1[(p1, p2, p3)]: dic_4_1[(p1, p2, p3)][p0] = 0\n",
        "        dic_4_1[(p1, p2, p3)][p0] += 1\n",
        "      if i>0 and i<L-2:\n",
        "        p1, p2, p3 = w[i-1], w[i+1], w[i+2] # *_**\n",
        "        if (p1, p2, p3) not in dic_4_2: dic_4_2[(p1, p2, p3)] = {}\n",
        "        if p0 not in dic_4_2[(p1, p2, p3)]: dic_4_2[(p1, p2, p3)][p0] = 0\n",
        "        dic_4_2[(p1, p2, p3)][p0] += 1\n",
        "      if i>1 and i<L-1:\n",
        "        p1, p2, p3 = w[i-2], w[i-1], w[i+1] # **_*\n",
        "        if (p1, p2, p3) not in dic_4_3: dic_4_3[(p1, p2, p3)] = {}\n",
        "        if p0 not in dic_4_3[(p1, p2, p3)]: dic_4_3[(p1, p2, p3)][p0] = 0\n",
        "        dic_4_3[(p1, p2, p3)][p0] += 1\n",
        "      if i>2:\n",
        "        p1, p2, p3 = w[i-3], w[i-2], w[i-1] # ***_\n",
        "        if (p1, p2, p3) not in dic_4_4: dic_4_4[(p1, p2, p3)] = {}\n",
        "        if p0 not in dic_4_4[(p1, p2, p3)]: dic_4_4[(p1, p2, p3)][p0] = 0\n",
        "        dic_4_4[(p1, p2, p3)][p0] += 1\n",
        "\n",
        "  normalize_dic(dic_4_1)\n",
        "  normalize_dic(dic_4_2)\n",
        "  normalize_dic(dic_4_3)\n",
        "  normalize_dic(dic_4_4)\n",
        "  return dic_4_1, dic_4_2, dic_4_3, dic_4_4\n",
        "\n",
        "# 5 gram\n",
        "def pattern04(train_words):\n",
        "  dic_5_1, dic_5_2, dic_5_3, dic_5_4, dic_5_5 = {}, {}, {}, {}, {}\n",
        "  for w in train_words:\n",
        "    L = len(w)\n",
        "    for i in range( L ):\n",
        "      p0 = w[i]\n",
        "      if i-4>=0: # ****_\n",
        "        p1, p2, p3, p4 = w[i-4], w[i-3], w[i-2], w[i-1]\n",
        "        if (p1, p2, p3, p4) not in dic_5_5: dic_5_5[(p1, p2, p3, p4)] = {}\n",
        "        if p0 not in dic_5_5[(p1, p2, p3, p4)]: dic_5_5[(p1, p2, p3, p4)][p0] = 0\n",
        "        dic_5_5[(p1, p2, p3, p4)][p0] += 1\n",
        "      if i-3>=0 and i+1<L: # ***_*\n",
        "        p1, p2, p3, p4 = w[i-3], w[i-2], w[i-1], w[i+1]\n",
        "        if (p1, p2, p3, p4) not in dic_5_4: dic_5_4[(p1, p2, p3, p4)] = {}\n",
        "        if p0 not in dic_5_4[(p1, p2, p3, p4)]: dic_5_4[(p1, p2, p3, p4)][p0] = 0\n",
        "        dic_5_4[(p1, p2, p3, p4)][p0] += 1\n",
        "      if i-2>=0 and i+2<L: # **_**\n",
        "        p1, p2, p3, p4 = w[i-2], w[i-1], w[i+1], w[i+2]\n",
        "        if (p1, p2, p3, p4) not in dic_5_3: dic_5_3[(p1, p2, p3, p4)] = {}\n",
        "        if p0 not in dic_5_3[(p1, p2, p3, p4)]: dic_5_3[(p1, p2, p3, p4)][p0] = 0\n",
        "        dic_5_3[(p1, p2, p3, p4)][p0] += 1\n",
        "      if i-1>=0 and i+3<L: # *_***\n",
        "        p1, p2, p3, p4 = w[i-1], w[i+1], w[i+2], w[i+3]\n",
        "        if (p1, p2, p3, p4) not in dic_5_2: dic_5_2[(p1, p2, p3, p4)] = {}\n",
        "        if p0 not in dic_5_2[(p1, p2, p3, p4)]: dic_5_2[(p1, p2, p3, p4)][p0] = 0\n",
        "        dic_5_2[(p1, p2, p3, p4)][p0] += 1\n",
        "      if i+4<L: # _****\n",
        "        p1, p2, p3, p4 = w[i+1], w[i+2], w[i+3], w[i+4]\n",
        "        if (p1, p2, p3, p4) not in dic_5_1: dic_5_1[(p1, p2, p3, p4)] = {}\n",
        "        if p0 not in dic_5_1[(p1, p2, p3, p4)]: dic_5_1[(p1, p2, p3, p4)][p0] = 0\n",
        "        dic_5_1[(p1, p2, p3, p4)][p0] += 1\n",
        "\n",
        "  normalize_dic(dic_5_1)\n",
        "  normalize_dic(dic_5_2)\n",
        "  normalize_dic(dic_5_3)\n",
        "  normalize_dic(dic_5_4)\n",
        "  normalize_dic(dic_5_5)\n",
        "  return dic_5_1, dic_5_2, dic_5_3, dic_5_4, dic_5_5"
      ],
      "metadata": {
        "id": "4EmcWt6Zti1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# for dict: key is char, and value is the probability (or frequency)\n",
        "# choose the char with the highest probability (or frequency)\n",
        "def choose_high_frequency(char_fre):\n",
        "  c, f = ' ', -1\n",
        "  for k in char_fre:\n",
        "    if char_fre[k]>f: c, f = k, char_fre[k]\n",
        "  return c\n",
        "\n",
        "# merge two dict\n",
        "# use the common key, and value is the sum\n",
        "def merge_two_dic01(d1, d2):\n",
        "  d = {}\n",
        "  for k in d1.keys():\n",
        "    if k in d2: d[k] = d1[k] + d2[k]\n",
        "  return d\n",
        "\n",
        "# check for n-gram\n",
        "def check_list(current_state, lst):\n",
        "  for i in lst:\n",
        "    if current_state[i] == '_': return False\n",
        "  return True\n",
        "\n",
        "# 2 gram:\n",
        "  # *_\n",
        "  # _*\n",
        "# 3 gram:\n",
        "  # **_\n",
        "  # *_*\n",
        "  # _**\n",
        "# 4 gram:\n",
        "  # **_*\n",
        "  # ***_\n",
        "  # *_**\n",
        "  # _***\n",
        "# 5 gram:\n",
        "  # **_**\n",
        "  # ***_*\n",
        "  # ****_\n",
        "  # *_***\n",
        "  # _****\n",
        "def N_grams(current_state, word_length):\n",
        "  # dic_2_1, dic_2_2,\n",
        "  # dic_3_1, dic_3_2, dic_3_3\n",
        "  # dic_4_1, dic_4_2, dic_4_3, dic_4_4\n",
        "  # dic_5_1, dic_5_2, dic_5_3, dic_5_4, dic_5_5\n",
        "\n",
        "  gram2, gram3, gram4 = {}, {}, {}\n",
        "  gram5 = {}\n",
        "  for i in range(word_length):\n",
        "    if current_state[i]=='_':\n",
        "      # 5 gram:\n",
        "      if i-4>=0: # ****_\n",
        "        if check_list(current_state, [i-4,i-3,i-2,i-1]):\n",
        "          p1, p2, p3, p4 = current_state[i-4], current_state[i-3], current_state[i-2], current_state[i-1]\n",
        "          if (p1, p2, p3, p4) in dic_5_5:\n",
        "            if not gram5: gram5 = dic_5_5[(p1, p2, p3, p4)]\n",
        "            else: gram5 = merge_two_dic01(gram5, dic_5_5[(p1, p2, p3, p4)])\n",
        "      if i-3>=0 and i+1<word_length: # ***_*\n",
        "        if check_list(current_state, [i-3,i-2,i-1,i+1]):\n",
        "          p1, p2, p3, p4 = current_state[i-3], current_state[i-2], current_state[i-1], current_state[i+1]\n",
        "          if (p1, p2, p3, p4) in dic_5_4:\n",
        "            if not gram5: gram5 = dic_5_4[(p1, p2, p3, p4)]\n",
        "            else: gram5 = merge_two_dic01(gram5, dic_5_4[(p1, p2, p3, p4)])\n",
        "      if i-2>=0 and i+2<word_length: # **_**\n",
        "        if check_list(current_state, [i-2,i-1,i+1,i+2]):\n",
        "          p1, p2, p3, p4 = current_state[i-2], current_state[i-1], current_state[i+1], current_state[i+2],\n",
        "          if (p1, p2, p3, p4) in dic_5_3:\n",
        "            if not gram5: gram5 = dic_5_3[(p1, p2, p3, p4)]\n",
        "            else: gram5 = merge_two_dic01(gram5, dic_5_3[(p1, p2, p3, p4)])\n",
        "      if i-1>=0 and i+3<word_length: # *_***\n",
        "        if check_list(current_state, [i-1,i+1,i+2,i+3]):\n",
        "          p1, p2, p3, p4 = current_state[i-1], current_state[i+1], current_state[i+2], current_state[i+3]\n",
        "          if (p1, p2, p3, p4) in dic_5_2:\n",
        "            if not gram5: gram5 = dic_5_2[(p1, p2, p3, p4)]\n",
        "            else: gram5 = merge_two_dic01(gram5, dic_5_2[(p1, p2, p3, p4)])\n",
        "      if i+4<word_length: # _****\n",
        "        if check_list(current_state, [i+1,i+2,i+3,i+4]):\n",
        "          p1, p2, p3, p4 = current_state[i+1], current_state[i+2], current_state[i+3], current_state[i+4]\n",
        "          if (p1, p2, p3, p4) in dic_5_1:\n",
        "            if not gram5: gram5 = dic_5_1[(p1, p2, p3, p4)]\n",
        "            else: gram5 = merge_two_dic01(gram5, dic_5_1[(p1, p2, p3, p4)])\n",
        "\n",
        "      # 4 gram: ***_\n",
        "      if i-3>=0 and current_state[i-3]!='_' and current_state[i-2]!='_' and current_state[i-1]!='_':\n",
        "        p1, p2, p3 = current_state[i-3], current_state[i-2], current_state[i-1]\n",
        "        if (p1, p2, p3) in dic_4_4:\n",
        "          if not gram4: gram4 = dic_4_4[(p1, p2, p3)]\n",
        "          else: gram4 = merge_two_dic01(gram4, dic_4_4[(p1, p2, p3)])\n",
        "      # 4 gram: **_*\n",
        "      if i-2>=0 and i+1<word_length and current_state[i-2]!='_' and current_state[i-1]!='_' and current_state[i+1]!='_' :\n",
        "        p1, p2, p3 = current_state[i-2], current_state[i-1], current_state[i+1]\n",
        "        if (p1, p2, p3) in dic_4_3:\n",
        "          if not gram4: gram4 = dic_4_3[(p1, p2, p3)]\n",
        "          else: gram4 = merge_two_dic01(gram4, dic_4_3[(p1, p2, p3)])\n",
        "      # 4 gram: *_**\n",
        "      if i-1>=0 and i+2<word_length and current_state[i-1]!='_' and current_state[i+1]!='_' and current_state[i+2]!='_' :\n",
        "        p1, p2, p3 = current_state[i-1], current_state[i+1], current_state[i+2]\n",
        "        if (p1, p2, p3) in dic_4_2:\n",
        "          if not gram4: gram4 = dic_4_2[(p1, p2, p3)]\n",
        "          else: gram4 = merge_two_dic01(gram4, dic_4_2[(p1, p2, p3)])\n",
        "      # 4 gram: _***\n",
        "      if i+3<word_length and current_state[i+1]!='_' and current_state[i+2]!='_' and current_state[i+3]!='_' :\n",
        "        p1, p2, p3 = current_state[i+1], current_state[i+2], current_state[i+3]\n",
        "        if (p1, p2, p3) in dic_4_1:\n",
        "          if not gram4: gram4 = dic_4_1[(p1, p2, p3)]\n",
        "          else: gram4 = merge_two_dic01(gram4, dic_4_1[(p1, p2, p3)])\n",
        "\n",
        "      # 3 gram: **_\n",
        "      if i-2>=0 and current_state[i-2]!='_' and current_state[i-1]!='_':\n",
        "        p1, p2 = current_state[i-2], current_state[i-1]\n",
        "        if (p1, p2) in dic_3_3:\n",
        "          if not gram3: gram3 = dic_3_3[(p1, p2)]\n",
        "          else: gram3 = merge_two_dic01(gram3, dic_3_3[(p1, p2)])\n",
        "      # 3 gram: *_*\n",
        "      if i-1>=0 and current_state[i-1]!='_' and i+1<word_length and current_state[i+1]!='_':\n",
        "        p1, p2 = current_state[i-1], current_state[i+1]\n",
        "        if (p1, p2) in dic_3_2:\n",
        "          if not gram3: gram3 = dic_3_2[(p1, p2)]\n",
        "          else: gram3 = merge_two_dic01(gram3, dic_3_2[(p1, p2)])\n",
        "      # 3 gram: _**\n",
        "      if i+2<word_length and current_state[i+1]!='_' and current_state[i+2]!='_':\n",
        "        p1, p2 = current_state[i+1], current_state[i+2]\n",
        "        if (p1, p2) in dic_3_1:\n",
        "          if not gram3: gram3 = dic_3_1[(p1, p2)]\n",
        "          else: gram3 = merge_two_dic01(gram3, dic_3_1[(p1, p2)])\n",
        "\n",
        "      # 2 gram: *_\n",
        "      if i-1>=0 and current_state[i-1]!='_':\n",
        "        p1 = current_state[i-1]\n",
        "        if p1 in dic_2_2:\n",
        "          if not gram2: gram2 = dic_2_2[p1]\n",
        "          else: gram2 = merge_two_dic01(gram2, dic_2_2[p1])\n",
        "      # 2 gram: _*\n",
        "      if i+1<word_length and current_state[i+1]!='_':\n",
        "        p2 = current_state[i+1]\n",
        "        if p2 in dic_2_1:\n",
        "          if not gram2: gram2 = dic_2_1[p2]\n",
        "          else: gram2 = merge_two_dic01(gram2, dic_2_1[p2])\n",
        "  return gram2, gram3, gram4, gram5\n",
        "\n",
        "def guess_char(word_length, history, current_state, char_set):\n",
        "  #\n",
        "  possible = []\n",
        "  c = ' '\n",
        "  #c = random.choices( ['a', 'o', 'e', 'i', 'u'] )[0]\n",
        "  preferred_dic = char_in_order_by_default.copy()\n",
        "  if word_length in char_in_order_by_length:\n",
        "    preferred_dic = char_in_order_by_length[word_length].copy()\n",
        "  for ch in history:\n",
        "    if ch in preferred_dic: preferred_dic.remove(ch)\n",
        "  possible = preferred_dic.copy()\n",
        "  c = preferred_dic.pop(0)\n",
        "\n",
        "    # the first guess\n",
        "  if not history: return c\n",
        "\n",
        "    # n-grams\n",
        "  gram2, gram3, gram4, gram5 = N_grams(current_state, word_length)\n",
        "  grams = merge_grams(gram2, gram3, gram4, gram5, history)\n",
        "  possible = grams\n",
        "\n",
        "  #if word_length>=12: gram4, gram5 = {}, {}\n",
        "  #else: gram5 = {}\n",
        "  #print(c, history, current_state)\n",
        "  #print( sorted(grams.items(), key=lambda x:x[1], reverse=True) )\n",
        "  if gram2: #  or gram3 or gram4 or gram5\n",
        "    c = choose_high_frequency(grams)\n",
        "\n",
        "  #print(c, history, current_state)\n",
        "  return c\n",
        "\n",
        "def merge_grams(gram2, gram3, gram4, gram5, history):\n",
        "  grams = dic_1_1.copy()\n",
        "\n",
        "  for k in grams:\n",
        "    grams[k] = grams[k] * 0.3 # 0.5, 0.8\n",
        "    if k in gram2: grams[k] += gram2[k]*0.5\n",
        "    if k in gram3: grams[k] += gram3[k]*1\n",
        "    if k in gram4: grams[k] += gram4[k]*2 #2.5\n",
        "    if k in gram5: grams[k] += gram5[k]*8 #5-8\n",
        "  for ch in history:\n",
        "    if ch in grams: del grams[ch]\n",
        "  return grams\n",
        "\n",
        "def weeken(grams, unlikely_dic, word_length):\n",
        "  count = 0\n",
        "  for (k,v) in sorted(unlikely_dic.items(), key=lambda x:x[1], reverse=True):\n",
        "    if k in grams:\n",
        "      grams[k] -= v*0.5\n",
        "      #grams[k] = grams[k] * weight0\n",
        "    count += 1\n",
        "    if count>word_length//2: break\n",
        "  return"
      ],
      "metadata": {
        "id": "1XE1H8D6tv-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_path):\n",
        "  f = open(file_path, 'r')\n",
        "  lns = []\n",
        "  for ln in f: lns.append(ln.strip())\n",
        "  return lns\n",
        "import random\n",
        "\n",
        "file_path = \"/content/words_250000_train.txt\"\n",
        "whole_words = read_file(file_path)\n",
        "random.shuffle(whole_words) # randomize those words\n",
        "\n",
        "split_ratio = 0.8\n",
        "Len = int(len(whole_words)*split_ratio)\n",
        "train_words = whole_words[:Len]\n",
        "test_words = whole_words[Len:]\n",
        "len(train_words), len(test_words)\n",
        "\n",
        "word_frequency = {'a':0,'b':0,'c':0,'d':0,'e':0,'f':0,'g':0,'h':0,'i':0,'j':0,'k':0,'l':0,'m':0,'n':0,'o':0,'p':0,'q':0,'r':0,'s':0,'t':0,'u':0,'v':0,'w':0,'x':0,'y':0,'z':0}\n",
        "for w in train_words:\n",
        "  for c in w: word_frequency[c] += 1\n",
        "word_frequency\n",
        "\n",
        "chars_in_order_by_frequency = sorted( word_frequency.items(), key=lambda x:x[1], reverse=True)\n",
        "chars_in_order_by_frequency = list(map(lambda x:x[0], chars_in_order_by_frequency))\n",
        "\n",
        "\n",
        "# statistics of the training data, calculate probabilities for n-grams\n",
        "dic_1_1 = pattern00(train_words)\n",
        "dic_2_1, dic_2_2 = pattern01(train_words)\n",
        "dic_3_1, dic_3_2, dic_3_3 = pattern02(train_words)\n",
        "dic_4_1, dic_4_2, dic_4_3, dic_4_4 = pattern03(train_words)\n",
        "dic_5_1, dic_5_2, dic_5_3, dic_5_4, dic_5_5 = pattern04(train_words)\n",
        "\n",
        "len_dic, char_in_order_by_length = statistics01(train_words)\n",
        "char_in_order_by_default = sorted(dic_1_1.items(), key=lambda x:x[1], reverse=True)\n",
        "char_in_order_by_default = list(map(lambda x:x[0], char_in_order_by_default))"
      ],
      "metadata": {
        "id": "iJ0of-5OXd5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "word:\n",
        "  length L,\n",
        "current state:\n",
        "  the number of \"_\": m\n",
        "  the number of chars: n\n",
        "    m + n = L\n",
        "  it can be expressed as dict\n",
        "history\n",
        "  suppose the guesses history is a list with length t,\n",
        "  then the remaining possible chars are 26-t\n",
        "maximum trying times:\n",
        "  L + 6\n",
        "current times:\n",
        "  how many times already tried: len( history )\n",
        "labels:\n",
        "  the remaining correct chars for this word\n",
        "\n",
        "for example:\n",
        "  word = \"hello\"\n",
        "  current state = \"h_ll_\" expressed as dict:\n",
        "      {\n",
        "        'a':0,'b':0,'c':0,'d':0,'e':0,'f':0,'g':0,\n",
        "        'h':1,\n",
        "        'i':0,'j':0,'k':0,\n",
        "        'l':2,\n",
        "        'm':0,'n':0,'o':0,'p':0,'q':0,'r':0,'s':0,'t':0,'u':0,'v':0,'w':0,'x':0,'y':0,'z':0,\n",
        "        '_':2\n",
        "      }\n",
        "    it can be expressed as vector by using its frequencies in order:\n",
        "      [\n",
        "        0,0,0,0,0,0,0,\n",
        "        1,\n",
        "        0,0,0,\n",
        "        2,\n",
        "        0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
        "        2\n",
        "       ]\n",
        "  history = ['a', 'b', 'c', 'h', 'l']\n",
        "    the remaining possible chars are:\n",
        "      ['d','e','f','g','i','j','k','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "  labels:\n",
        "    'e' or 'o'\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "def state_to_vector(current_state):\n",
        "  #dic = {'a':0,'b':0,'c':0,'d':0,'e':0,'f':0,'g':0,'h':0,'i':0,'j':0,'k':0,'l':0,'m':0,'n':0,'o':0,'p':0,'q':0,'r':0,'s':0,'t':0,'u':0,'v':0,'w':0,'x':0,'y':0,'z':0,'_':0}\n",
        "  # length is 27: a-z, _\n",
        "  vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "  for c in current_state:\n",
        "    if c=='_': vec[-1] += 1\n",
        "    else: vec[ ord(c)-ord('a') ] += 1\n",
        "  return vec\n",
        "\n",
        "# the complement of the already guessed chars\n",
        "def history_to_vector(history):\n",
        "  #dic = {'a':1,'b':1,'c':1,'d':1,'e':1,'f':1,'g':1,'h':1,'i':1,'j':1,'k':1,'l':1,'m':1,'n':1,'o':1,'p':1,'q':1,'r':1,'s':1,'t':1,'u':1,'v':1,'w':1,'x':1,'y':1,'z':1}\n",
        "  vec = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
        "  for c in history: vec[ ord(c)-ord('a') ] = 0\n",
        "  return vec\n",
        "\n",
        "def char_to_vector(c):\n",
        "  #dic = {'a':0,'b':0,'c':0,'d':0,'e':0,'f':0,'g':0,'h':0,'i':0,'j':0,'k':0,'l':0,'m':0,'n':0,'o':0,'p':0,'q':0,'r':0,'s':0,'t':0,'u':0,'v':0,'w':0,'x':0,'y':0,'z':0}\n",
        "  vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "  if c==\"_\": return vec\n",
        "  vec[ ord(c)-ord('a') ] = 1\n",
        "  return vec\n",
        "\n",
        "def dic_to_list(grams):\n",
        "  vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "  for k in grams:\n",
        "    vec[ ord(k)-ord('a') ] = grams[k]\n",
        "  return vec\n",
        "\n",
        "def get_labels(word, current_state):\n",
        "  s = set()\n",
        "  for i in range(len(word)):\n",
        "    if word[i]!=current_state[i]:\n",
        "      s.add( word[i] )\n",
        "  return s\n",
        "\n",
        "def get_features(current_state, history, current_times, max_times):\n",
        "  vec1 = state_to_vector(current_state)\n",
        "  vec2 = history_to_vector(history)\n",
        "  return vec1 + vec2 + [current_times, max_times]\n",
        "\n",
        "def get_features02(current_state, ch, current_times, max_times):\n",
        "  vec1 = state_to_vector(current_state)\n",
        "  vec2 = char_to_vector(ch)\n",
        "  return vec1 + vec2 + [current_times, max_times]\n",
        "\n",
        "def get_features03(current_state, history, current_times, max_times):\n",
        "  vec1 = state_to_vector(current_state)\n",
        "  for c in history: vec1[ ord(c)-ord('a') ] = -1\n",
        "  return vec1 + [current_times, max_times]\n",
        "\n",
        "def get_samples(word):\n",
        "  word_length = len(word)\n",
        "  current_state = ['_'] * word_length\n",
        "  history = []\n",
        "  max_times = word_length + 6\n",
        "  current_times = 0\n",
        "  # S = sorted(list(set(word)))\n",
        "  S = []\n",
        "  for c in chars_in_order_by_frequency:\n",
        "    if c in set(word): S.append( c )\n",
        "  # 55, 29\n",
        "  X_data, y_data = np.empty((0, 53), int), np.empty((0, 26), int)\n",
        "  ch = \"_\"\n",
        "\n",
        "  while S and current_times<max_times:\n",
        "    features = get_features(current_state, history, current_times, max_times)\n",
        "    gram2, gram3, gram4, gram5 = N_grams(current_state, word_length)\n",
        "    grams = merge_grams(gram2, gram3, gram4, gram5, history)\n",
        "    #features = get_features02(current_state, ch, current_times, max_times)\n",
        "    #features = get_features03(current_state, history, current_times, max_times)\n",
        "    #ch = random.choices( list(S) )[0] # one of the correct label\n",
        "    ch = S.pop(0) #S.remove(ch)\n",
        "    label = char_to_vector(ch)\n",
        "    vec1 = state_to_vector(current_state)\n",
        "    vec2 = dic_to_list(grams)\n",
        "    features = vec1 + vec2\n",
        "    #print( vec1 )\n",
        "    #print(len(vec2), vec2)\n",
        "    #print( label )\n",
        "\n",
        "\n",
        "    #yield (features, label)\n",
        "    #X_data.append( features )\n",
        "    #y_data.append( label )\n",
        "    redundancy_features = [features]\n",
        "    redundancy_labels = [label]\n",
        "    X_data = np.append(X_data, redundancy_features, axis=0)\n",
        "    y_data = np.append(y_data, redundancy_labels, axis=0)\n",
        "    #print( history, ch )\n",
        "\n",
        "    history.append(ch)\n",
        "    for i in range(word_length):\n",
        "      if word[i]==ch and current_state[i]!=ch:\n",
        "        current_state[i] = ch\n",
        "\n",
        "    current_times += 1\n",
        "  return X_data, y_data"
      ],
      "metadata": {
        "id": "KGNXzI08XoRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = train_words[:]\n",
        "\n",
        "training_X = np.vstack( tuple( map( lambda w: get_samples(w)[0], words ) ) )\n",
        "training_y = np.vstack( tuple( map( lambda w: get_samples(w)[1], words ) ) )\n",
        "training_X.shape, training_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQcAEkzuXuGM",
        "outputId": "a92a4bb0-af8e-4b48-9225-188e0588502a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1344732, 53), (1344732, 26))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras import optimizers\n",
        "\n",
        "seed = 10\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "NELxNXyrYJSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256, activation='relu', input_dim=53)) # 55, 29\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dense(32, activation='relu'))\n",
        "  model.add(Dense(26, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "model = create_model()\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "      optimizer = optimizers.Adam(learning_rate=0.001),\n",
        "      metrics = ['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_X, training_y, epochs=300, batch_size=128)\n",
        "#model.evaluate(testing_X, testing_y)\n",
        "#predictions = model.predict( testing_X )\n",
        "#labels_predicted = np.argmax(predictions, axis=1)\n",
        "#labels_predicted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Hbo4Wnbijj5",
        "outputId": "40ddfa47-a519-40b2-e2d6-2c4020e61720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 256)               13824     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 26)                858       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 57914 (226.23 KB)\n",
            "Trainable params: 57914 (226.23 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.1887 - accuracy: 0.5755\n",
            "Epoch 2/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.1252 - accuracy: 0.5899\n",
            "Epoch 3/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.1118 - accuracy: 0.5932\n",
            "Epoch 4/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.1037 - accuracy: 0.5957\n",
            "Epoch 5/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0981 - accuracy: 0.5969\n",
            "Epoch 6/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0939 - accuracy: 0.5983\n",
            "Epoch 7/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0901 - accuracy: 0.5996\n",
            "Epoch 8/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0872 - accuracy: 0.6005\n",
            "Epoch 9/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0846 - accuracy: 0.6009\n",
            "Epoch 10/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0824 - accuracy: 0.6020\n",
            "Epoch 11/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0802 - accuracy: 0.6026\n",
            "Epoch 12/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0785 - accuracy: 0.6034\n",
            "Epoch 13/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0767 - accuracy: 0.6038\n",
            "Epoch 14/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0753 - accuracy: 0.6044\n",
            "Epoch 15/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0740 - accuracy: 0.6046\n",
            "Epoch 16/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0728 - accuracy: 0.6049\n",
            "Epoch 17/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0716 - accuracy: 0.6055\n",
            "Epoch 18/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0704 - accuracy: 0.6060\n",
            "Epoch 19/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0691 - accuracy: 0.6063\n",
            "Epoch 20/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0684 - accuracy: 0.6066\n",
            "Epoch 21/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0673 - accuracy: 0.6072\n",
            "Epoch 22/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0667 - accuracy: 0.6071\n",
            "Epoch 23/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0662 - accuracy: 0.6074\n",
            "Epoch 24/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0651 - accuracy: 0.6081\n",
            "Epoch 25/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0645 - accuracy: 0.6080\n",
            "Epoch 26/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0634 - accuracy: 0.6085\n",
            "Epoch 27/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0630 - accuracy: 0.6089\n",
            "Epoch 28/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0626 - accuracy: 0.6087\n",
            "Epoch 29/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0622 - accuracy: 0.6090\n",
            "Epoch 30/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0611 - accuracy: 0.6092\n",
            "Epoch 31/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0609 - accuracy: 0.6095\n",
            "Epoch 32/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0602 - accuracy: 0.6092\n",
            "Epoch 33/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0597 - accuracy: 0.6097\n",
            "Epoch 34/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0597 - accuracy: 0.6099\n",
            "Epoch 35/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0587 - accuracy: 0.6100\n",
            "Epoch 36/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0585 - accuracy: 0.6102\n",
            "Epoch 37/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0578 - accuracy: 0.6104\n",
            "Epoch 38/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0578 - accuracy: 0.6106\n",
            "Epoch 39/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0572 - accuracy: 0.6107\n",
            "Epoch 40/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0569 - accuracy: 0.6106\n",
            "Epoch 41/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0565 - accuracy: 0.6110\n",
            "Epoch 42/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0561 - accuracy: 0.6109\n",
            "Epoch 43/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0555 - accuracy: 0.6114\n",
            "Epoch 44/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0553 - accuracy: 0.6115\n",
            "Epoch 45/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0549 - accuracy: 0.6117\n",
            "Epoch 46/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0543 - accuracy: 0.6117\n",
            "Epoch 47/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0542 - accuracy: 0.6116\n",
            "Epoch 48/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0537 - accuracy: 0.6121\n",
            "Epoch 49/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0538 - accuracy: 0.6120\n",
            "Epoch 50/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0535 - accuracy: 0.6124\n",
            "Epoch 51/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0530 - accuracy: 0.6123\n",
            "Epoch 52/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0527 - accuracy: 0.6126\n",
            "Epoch 53/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0524 - accuracy: 0.6125\n",
            "Epoch 54/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0526 - accuracy: 0.6124\n",
            "Epoch 55/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0522 - accuracy: 0.6125\n",
            "Epoch 56/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0520 - accuracy: 0.6129\n",
            "Epoch 57/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0517 - accuracy: 0.6131\n",
            "Epoch 58/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0518 - accuracy: 0.6132\n",
            "Epoch 59/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0516 - accuracy: 0.6128\n",
            "Epoch 60/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0510 - accuracy: 0.6131\n",
            "Epoch 61/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0509 - accuracy: 0.6131\n",
            "Epoch 62/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0509 - accuracy: 0.6133\n",
            "Epoch 63/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0507 - accuracy: 0.6135\n",
            "Epoch 64/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0505 - accuracy: 0.6134\n",
            "Epoch 65/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0500 - accuracy: 0.6137\n",
            "Epoch 66/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0503 - accuracy: 0.6135\n",
            "Epoch 67/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0498 - accuracy: 0.6137\n",
            "Epoch 68/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0500 - accuracy: 0.6137\n",
            "Epoch 69/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0499 - accuracy: 0.6138\n",
            "Epoch 70/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0498 - accuracy: 0.6138\n",
            "Epoch 71/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0495 - accuracy: 0.6140\n",
            "Epoch 72/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0494 - accuracy: 0.6140\n",
            "Epoch 73/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0492 - accuracy: 0.6139\n",
            "Epoch 74/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0490 - accuracy: 0.6140\n",
            "Epoch 75/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0488 - accuracy: 0.6143\n",
            "Epoch 76/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0487 - accuracy: 0.6143\n",
            "Epoch 77/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0485 - accuracy: 0.6143\n",
            "Epoch 78/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0485 - accuracy: 0.6143\n",
            "Epoch 79/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0477 - accuracy: 0.6142\n",
            "Epoch 80/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0485 - accuracy: 0.6145\n",
            "Epoch 81/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0480 - accuracy: 0.6147\n",
            "Epoch 82/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0476 - accuracy: 0.6145\n",
            "Epoch 83/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0478 - accuracy: 0.6145\n",
            "Epoch 84/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0481 - accuracy: 0.6144\n",
            "Epoch 85/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0474 - accuracy: 0.6146\n",
            "Epoch 86/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0482 - accuracy: 0.6143\n",
            "Epoch 87/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0471 - accuracy: 0.6148\n",
            "Epoch 88/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0471 - accuracy: 0.6148\n",
            "Epoch 89/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0469 - accuracy: 0.6150\n",
            "Epoch 90/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0473 - accuracy: 0.6150\n",
            "Epoch 91/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0468 - accuracy: 0.6154\n",
            "Epoch 92/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0469 - accuracy: 0.6149\n",
            "Epoch 93/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0471 - accuracy: 0.6153\n",
            "Epoch 94/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0465 - accuracy: 0.6151\n",
            "Epoch 95/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0467 - accuracy: 0.6152\n",
            "Epoch 96/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0464 - accuracy: 0.6151\n",
            "Epoch 97/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0463 - accuracy: 0.6150\n",
            "Epoch 98/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0456 - accuracy: 0.6152\n",
            "Epoch 99/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0458 - accuracy: 0.6155\n",
            "Epoch 100/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0464 - accuracy: 0.6155\n",
            "Epoch 101/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0454 - accuracy: 0.6155\n",
            "Epoch 102/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0457 - accuracy: 0.6151\n",
            "Epoch 103/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0454 - accuracy: 0.6154\n",
            "Epoch 104/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0455 - accuracy: 0.6153\n",
            "Epoch 105/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0461 - accuracy: 0.6156\n",
            "Epoch 106/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0454 - accuracy: 0.6154\n",
            "Epoch 107/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0451 - accuracy: 0.6154\n",
            "Epoch 108/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0451 - accuracy: 0.6156\n",
            "Epoch 109/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0453 - accuracy: 0.6155\n",
            "Epoch 110/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0451 - accuracy: 0.6158\n",
            "Epoch 111/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0450 - accuracy: 0.6160\n",
            "Epoch 112/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0452 - accuracy: 0.6156\n",
            "Epoch 113/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0449 - accuracy: 0.6162\n",
            "Epoch 114/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0449 - accuracy: 0.6160\n",
            "Epoch 115/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0446 - accuracy: 0.6158\n",
            "Epoch 116/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0449 - accuracy: 0.6158\n",
            "Epoch 117/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0440 - accuracy: 0.6163\n",
            "Epoch 118/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0440 - accuracy: 0.6158\n",
            "Epoch 119/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0444 - accuracy: 0.6160\n",
            "Epoch 120/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0442 - accuracy: 0.6160\n",
            "Epoch 121/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0444 - accuracy: 0.6161\n",
            "Epoch 122/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0442 - accuracy: 0.6161\n",
            "Epoch 123/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0447 - accuracy: 0.6163\n",
            "Epoch 124/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0445 - accuracy: 0.6162\n",
            "Epoch 125/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0442 - accuracy: 0.6160\n",
            "Epoch 126/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0438 - accuracy: 0.6162\n",
            "Epoch 127/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0441 - accuracy: 0.6162\n",
            "Epoch 128/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0438 - accuracy: 0.6168\n",
            "Epoch 129/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0441 - accuracy: 0.6159\n",
            "Epoch 130/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0439 - accuracy: 0.6164\n",
            "Epoch 131/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0440 - accuracy: 0.6166\n",
            "Epoch 132/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0442 - accuracy: 0.6162\n",
            "Epoch 133/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0445 - accuracy: 0.6163\n",
            "Epoch 134/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0439 - accuracy: 0.6166\n",
            "Epoch 135/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0431 - accuracy: 0.6167\n",
            "Epoch 136/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0433 - accuracy: 0.6163\n",
            "Epoch 137/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0437 - accuracy: 0.6163\n",
            "Epoch 138/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0441 - accuracy: 0.6164\n",
            "Epoch 139/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0437 - accuracy: 0.6163\n",
            "Epoch 140/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0437 - accuracy: 0.6166\n",
            "Epoch 141/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0431 - accuracy: 0.6164\n",
            "Epoch 142/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0432 - accuracy: 0.6163\n",
            "Epoch 143/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0435 - accuracy: 0.6165\n",
            "Epoch 144/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0430 - accuracy: 0.6166\n",
            "Epoch 145/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0435 - accuracy: 0.6167\n",
            "Epoch 146/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0429 - accuracy: 0.6165\n",
            "Epoch 147/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0441 - accuracy: 0.6169\n",
            "Epoch 148/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0430 - accuracy: 0.6167\n",
            "Epoch 149/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0433 - accuracy: 0.6164\n",
            "Epoch 150/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0436 - accuracy: 0.6169\n",
            "Epoch 151/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0433 - accuracy: 0.6168\n",
            "Epoch 152/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0433 - accuracy: 0.6165\n",
            "Epoch 153/300\n",
            "10506/10506 [==============================] - 34s 3ms/step - loss: 1.0427 - accuracy: 0.6168\n",
            "Epoch 154/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0432 - accuracy: 0.6168\n",
            "Epoch 155/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0429 - accuracy: 0.6168\n",
            "Epoch 156/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0431 - accuracy: 0.6170\n",
            "Epoch 157/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0432 - accuracy: 0.6168\n",
            "Epoch 158/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0425 - accuracy: 0.6169\n",
            "Epoch 159/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0431 - accuracy: 0.6169\n",
            "Epoch 160/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0423 - accuracy: 0.6169\n",
            "Epoch 161/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0427 - accuracy: 0.6168\n",
            "Epoch 162/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0428 - accuracy: 0.6169\n",
            "Epoch 163/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0427 - accuracy: 0.6171\n",
            "Epoch 164/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0429 - accuracy: 0.6170\n",
            "Epoch 165/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0425 - accuracy: 0.6169\n",
            "Epoch 166/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0429 - accuracy: 0.6169\n",
            "Epoch 167/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0424 - accuracy: 0.6169\n",
            "Epoch 168/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0425 - accuracy: 0.6170\n",
            "Epoch 169/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0423 - accuracy: 0.6172\n",
            "Epoch 170/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0421 - accuracy: 0.6172\n",
            "Epoch 171/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0428 - accuracy: 0.6170\n",
            "Epoch 172/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0430 - accuracy: 0.6173\n",
            "Epoch 173/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0430 - accuracy: 0.6170\n",
            "Epoch 174/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0424 - accuracy: 0.6169\n",
            "Epoch 175/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0427 - accuracy: 0.6170\n",
            "Epoch 176/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0425 - accuracy: 0.6171\n",
            "Epoch 177/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0435 - accuracy: 0.6167\n",
            "Epoch 178/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0428 - accuracy: 0.6169\n",
            "Epoch 179/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0431 - accuracy: 0.6171\n",
            "Epoch 180/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0432 - accuracy: 0.6170\n",
            "Epoch 181/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0424 - accuracy: 0.6169\n",
            "Epoch 182/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0422 - accuracy: 0.6173\n",
            "Epoch 183/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0424 - accuracy: 0.6172\n",
            "Epoch 184/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0422 - accuracy: 0.6175\n",
            "Epoch 185/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0421 - accuracy: 0.6174\n",
            "Epoch 186/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0431 - accuracy: 0.6172\n",
            "Epoch 187/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0422 - accuracy: 0.6174\n",
            "Epoch 188/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0419 - accuracy: 0.6174\n",
            "Epoch 189/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0415 - accuracy: 0.6174\n",
            "Epoch 190/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0422 - accuracy: 0.6174\n",
            "Epoch 191/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0418 - accuracy: 0.6174\n",
            "Epoch 192/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0426 - accuracy: 0.6172\n",
            "Epoch 193/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0422 - accuracy: 0.6172\n",
            "Epoch 194/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0421 - accuracy: 0.6174\n",
            "Epoch 195/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0419 - accuracy: 0.6173\n",
            "Epoch 196/300\n",
            "10506/10506 [==============================] - 34s 3ms/step - loss: 1.0422 - accuracy: 0.6172\n",
            "Epoch 197/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0425 - accuracy: 0.6173\n",
            "Epoch 198/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0417 - accuracy: 0.6172\n",
            "Epoch 199/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0432 - accuracy: 0.6171\n",
            "Epoch 200/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0423 - accuracy: 0.6171\n",
            "Epoch 201/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0427 - accuracy: 0.6172\n",
            "Epoch 202/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0414 - accuracy: 0.6176\n",
            "Epoch 203/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0422 - accuracy: 0.6172\n",
            "Epoch 204/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0417 - accuracy: 0.6172\n",
            "Epoch 205/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0421 - accuracy: 0.6176\n",
            "Epoch 206/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0418 - accuracy: 0.6175\n",
            "Epoch 207/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0427 - accuracy: 0.6172\n",
            "Epoch 208/300\n",
            "10506/10506 [==============================] - 34s 3ms/step - loss: 1.0419 - accuracy: 0.6173\n",
            "Epoch 209/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0413 - accuracy: 0.6175\n",
            "Epoch 210/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0420 - accuracy: 0.6175\n",
            "Epoch 211/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0415 - accuracy: 0.6176\n",
            "Epoch 212/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0434 - accuracy: 0.6176\n",
            "Epoch 213/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0417 - accuracy: 0.6175\n",
            "Epoch 214/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0427 - accuracy: 0.6173\n",
            "Epoch 215/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0417 - accuracy: 0.6176\n",
            "Epoch 216/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0418 - accuracy: 0.6175\n",
            "Epoch 217/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0417 - accuracy: 0.6175\n",
            "Epoch 218/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0422 - accuracy: 0.6175\n",
            "Epoch 219/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0417 - accuracy: 0.6178\n",
            "Epoch 220/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0414 - accuracy: 0.6179\n",
            "Epoch 221/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0418 - accuracy: 0.6174\n",
            "Epoch 222/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0420 - accuracy: 0.6173\n",
            "Epoch 223/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0417 - accuracy: 0.6175\n",
            "Epoch 224/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0428 - accuracy: 0.6179\n",
            "Epoch 225/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0420 - accuracy: 0.6176\n",
            "Epoch 226/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0419 - accuracy: 0.6173\n",
            "Epoch 227/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0422 - accuracy: 0.6177\n",
            "Epoch 228/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0414 - accuracy: 0.6176\n",
            "Epoch 229/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0419 - accuracy: 0.6174\n",
            "Epoch 230/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0421 - accuracy: 0.6175\n",
            "Epoch 231/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0420 - accuracy: 0.6171\n",
            "Epoch 232/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0439 - accuracy: 0.6176\n",
            "Epoch 233/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0416 - accuracy: 0.6174\n",
            "Epoch 234/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0416 - accuracy: 0.6175\n",
            "Epoch 235/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0424 - accuracy: 0.6174\n",
            "Epoch 236/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0418 - accuracy: 0.6177\n",
            "Epoch 237/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0428 - accuracy: 0.6175\n",
            "Epoch 238/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0420 - accuracy: 0.6178\n",
            "Epoch 239/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0420 - accuracy: 0.6176\n",
            "Epoch 240/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0413 - accuracy: 0.6178\n",
            "Epoch 241/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0416 - accuracy: 0.6180\n",
            "Epoch 242/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0418 - accuracy: 0.6176\n",
            "Epoch 243/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0414 - accuracy: 0.6177\n",
            "Epoch 244/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0422 - accuracy: 0.6177\n",
            "Epoch 245/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0417 - accuracy: 0.6180\n",
            "Epoch 246/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0435 - accuracy: 0.6179\n",
            "Epoch 247/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0412 - accuracy: 0.6179\n",
            "Epoch 248/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0417 - accuracy: 0.6179\n",
            "Epoch 249/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0420 - accuracy: 0.6175\n",
            "Epoch 250/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0412 - accuracy: 0.6181\n",
            "Epoch 251/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0411 - accuracy: 0.6180\n",
            "Epoch 252/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0428 - accuracy: 0.6175\n",
            "Epoch 253/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0416 - accuracy: 0.6180\n",
            "Epoch 254/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0429 - accuracy: 0.6178\n",
            "Epoch 255/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0422 - accuracy: 0.6177\n",
            "Epoch 256/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0416 - accuracy: 0.6178\n",
            "Epoch 257/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0423 - accuracy: 0.6178\n",
            "Epoch 258/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0422 - accuracy: 0.6175\n",
            "Epoch 259/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0427 - accuracy: 0.6179\n",
            "Epoch 260/300\n",
            "10506/10506 [==============================] - 30s 3ms/step - loss: 1.0428 - accuracy: 0.6177\n",
            "Epoch 261/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0415 - accuracy: 0.6178\n",
            "Epoch 262/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0419 - accuracy: 0.6177\n",
            "Epoch 263/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0425 - accuracy: 0.6177\n",
            "Epoch 264/300\n",
            "10506/10506 [==============================] - 34s 3ms/step - loss: 1.0418 - accuracy: 0.6178\n",
            "Epoch 265/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0421 - accuracy: 0.6180\n",
            "Epoch 266/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0417 - accuracy: 0.6178\n",
            "Epoch 267/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0431 - accuracy: 0.6178\n",
            "Epoch 268/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0418 - accuracy: 0.6179\n",
            "Epoch 269/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0411 - accuracy: 0.6178\n",
            "Epoch 270/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0419 - accuracy: 0.6179\n",
            "Epoch 271/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0416 - accuracy: 0.6181\n",
            "Epoch 272/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0410 - accuracy: 0.6180\n",
            "Epoch 273/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0408 - accuracy: 0.6181\n",
            "Epoch 274/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0423 - accuracy: 0.6176\n",
            "Epoch 275/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0417 - accuracy: 0.6179\n",
            "Epoch 276/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0421 - accuracy: 0.6179\n",
            "Epoch 277/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0424 - accuracy: 0.6178\n",
            "Epoch 278/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0402 - accuracy: 0.6183\n",
            "Epoch 279/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0416 - accuracy: 0.6179\n",
            "Epoch 280/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0408 - accuracy: 0.6181\n",
            "Epoch 281/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0407 - accuracy: 0.6180\n",
            "Epoch 282/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0411 - accuracy: 0.6178\n",
            "Epoch 283/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0440 - accuracy: 0.6178\n",
            "Epoch 284/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0423 - accuracy: 0.6177\n",
            "Epoch 285/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0421 - accuracy: 0.6179\n",
            "Epoch 286/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0416 - accuracy: 0.6181\n",
            "Epoch 287/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0404 - accuracy: 0.6180\n",
            "Epoch 288/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0417 - accuracy: 0.6177\n",
            "Epoch 289/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0422 - accuracy: 0.6178\n",
            "Epoch 290/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0428 - accuracy: 0.6178\n",
            "Epoch 291/300\n",
            "10506/10506 [==============================] - 33s 3ms/step - loss: 1.0417 - accuracy: 0.6182\n",
            "Epoch 292/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0417 - accuracy: 0.6181\n",
            "Epoch 293/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0414 - accuracy: 0.6181\n",
            "Epoch 294/300\n",
            "10506/10506 [==============================] - 31s 3ms/step - loss: 1.0418 - accuracy: 0.6184\n",
            "Epoch 295/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0411 - accuracy: 0.6180\n",
            "Epoch 296/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0454 - accuracy: 0.6179\n",
            "Epoch 297/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0410 - accuracy: 0.6182\n",
            "Epoch 298/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0413 - accuracy: 0.6181\n",
            "Epoch 299/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0416 - accuracy: 0.6179\n",
            "Epoch 300/300\n",
            "10506/10506 [==============================] - 32s 3ms/step - loss: 1.0403 - accuracy: 0.6182\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e66682dac50>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model06.keras\")\n",
        "# loaded_model = keras.saving.load_model(\"model.keras\")"
      ],
      "metadata": {
        "id": "1HGzaeB-l0_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_high_frequency(char_fre):\n",
        "  c, f = ' ', -1\n",
        "  for k in char_fre:\n",
        "    if char_fre[k]>f:\n",
        "      c, f = k, char_fre[k]\n",
        "  return c\n",
        "\n",
        "def play_game_ML(word):\n",
        "  word_length, try_times, current_state = start_game(word, 6) # step 1\n",
        "  #char_set = {'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'}\n",
        "  char_set = word_frequency.copy()\n",
        "  history, current_times, winning = [], 0, 0\n",
        "\n",
        "  c = '_'\n",
        "  while current_times < try_times:\n",
        "    #c = guess_function(word_length, history, current_state, char_set)\n",
        "    features = get_features(current_state, history, current_times, try_times)\n",
        "    #features = get_features02(current_state, c, current_times, try_times)\n",
        "    #features = get_features03(current_state, history, current_times, try_times)\n",
        "    gram2, gram3, gram4, gram5 = N_grams(current_state, word_length)\n",
        "    grams = merge_grams(gram2, gram3, gram4, gram5, history)\n",
        "    vec1 = state_to_vector(current_state)\n",
        "    vec2 = dic_to_list(grams)\n",
        "    features = vec1 + vec2\n",
        "\n",
        "\n",
        "    label = model.predict( np.array([features]), verbose=0 )[0]\n",
        "    char_idx = np.argmax( label )\n",
        "    c = chr( ord('a')+char_idx )\n",
        "    #print(char_idx, c)\n",
        "    if c not in char_set:\n",
        "      #c = random.choices( list(char_set) )[0]\n",
        "      c = choose_high_frequency( char_set )\n",
        "\n",
        "    history.append(c) # add in the newly guessed char\n",
        "    #char_set.remove(c)\n",
        "    del char_set[c]\n",
        "\n",
        "    f = update( word, current_state, set(history) ) # step 2\n",
        "    #if f: print(\"guess correctly\")\n",
        "    #else: print(\"no such char\")\n",
        "\n",
        "    blanks = evaluate(current_state) # step 3\n",
        "    if blanks==0:\n",
        "      winning = 1\n",
        "      break\n",
        "    current_times += 1\n",
        "  return winning\n",
        "\n",
        "c1, c2 = 0, 0\n",
        "for word in test_words[:2000]:\n",
        "  #print(word)\n",
        "  winning = play_game_ML(word)\n",
        "  if winning:\n",
        "    c2 += 1\n",
        "    #print(word)\n",
        "  c1 += 1\n",
        "c1, c2, c2/c1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ustDfOiNi5nN",
        "outputId": "db33d804-dc4b-401c-b4d6-d7a4e273c9dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 1170, 0.585)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "win_list = list( map( lambda w: play_game_ML(w), test_words[-2000:] ) )\n",
        "sum(win_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj7srlplwpRc",
        "outputId": "1a19664e-388a-45c0-9260-2b8340961187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1118"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "def play_game_ML_fast(test_words, idx):\n",
        "  cases = list( map( lambda w: play_game_ML(w), test_words[idx:idx+10] ) )\n",
        "  print(sum(cases))\n",
        "  return\n",
        "\n",
        "import threading\n",
        "\n",
        "all_threads = []\n",
        "for i in range(10):\n",
        "  t = threading.Thread(target=play_game_ML_fast, args=(test_words, i*10))\n",
        "  all_threads.append( t )\n",
        "\n",
        "for i in range(10):\n",
        "  all_threads[i].start()\n",
        "\n",
        "for i in range(10):\n",
        "  all_threads[i].join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xDsoqRsrK1x",
        "outputId": "fa7111b4-9f8e-49cb-ebfc-7a2fe786874c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "5\n",
            "3\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "5\n",
            "2\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AdlehdwJ1ZTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KO5pN4Kgtvlo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}